Use a fixed number of layers: 1 - S^n - 1
Hyper-parameter:
- Learning Rate
    Too high - overshoot the minimum (overfitting)
    Too low - too many iterations to get to the minimum
- Number of neurons in hidden-layer
- Number of epochs of training
- Mini-Batch size

Gradient Descent (GD) requires a cost-function, a gradient (dj/dw - derivative of 
cost-function).
Due to the size of our inputs, experiment with Batch-GD: to get a smooth graph

Goals/Experiments:
1. Determine the best learning rate 
 - Keep the epoches constant of 200, number of neurons in the hidden layer 200 
   and batch-size of 1.

2. Determine the number of epoches
3. Determine the number of a neuron
4. Determine the size of a batch-size





