Hyper-parameter:
- Learning Rate
    Too high - overshoot the minimum (overfitting)
    Too low - too many iterations to get to the minimum

- Gradient Descent (GD) requires a cost-function, a gradient (dj/dw - derivative of 
cost-function).
    Due to the size of our inputs, experiment with Batch-GD: to get a smooth graph

The cost function used is mean-squared error

%hyperparamaters:
%1. Learning Rates
    %To find the optimal rates: Experiment with multiple rates 
        %(Too High - may never reach a minimum)
        %(Too Low  - may get stuck at a local minimum)
        %Starting with 0.1 learning rate
        %Testing with 0.25 learning rate

%feedforward
%performance index: mean square error = 

%Starting with one layers
%Testing with two layers
%Testing with three layers
